{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/gagandeep/spark/spark-2.4.5-bin-hadoop2.7\")\n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('logr').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToFile = \"/media/gagandeep/2E92405C92402AA3/Work/Codes/PythonCodes/SparkLesson/DataFiles/Spark_for_Machine_Learning/Logistic_Regression/titanic.csv\"\n",
    "data = spark.read.csv(pathToFile, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['PassengerId',\n 'Survived',\n 'Pclass',\n 'Name',\n 'Sex',\n 'Age',\n 'SibSp',\n 'Parch',\n 'Ticket',\n 'Fare',\n 'Cabin',\n 'Embarked']"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- PassengerId: integer (nullable = true)\n |-- Survived: integer (nullable = true)\n |-- Pclass: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: integer (nullable = true)\n |-- Parch: integer (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Cabin: string (nullable = true)\n |-- Embarked: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCols = ['Survived',\n",
    " 'Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data.select(myCols).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderIndexer = StringIndexer(inputCol='Sex', outputCol='sex_indexed')\n",
    "genderEncoder = OneHotEncoder(inputCol='sex_indexed', outputCol='sex_vec')\n",
    "embarkedIndexer = StringIndexer(inputCol='Embarked', outputCol='embarked_indexed')\n",
    "embarkedEncoder = OneHotEncoder(inputCol='embarked_indexed', outputCol='embarked_vec')\n",
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'sex_vec',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'embarked_vec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Spark Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[genderIndexer,embarkedIndexer,\n",
    "                           genderEncoder,embarkedEncoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------+----------+\n|Survived|prediction|\n+--------+----------+\n|       0|       0.0|\n|       0|       1.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       1.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       1.0|\n|       0|       1.0|\n|       0|       1.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n+--------+----------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8115915697674418"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "AUC = my_eval.evaluate(results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 2\n",
    "pathToFile = \"/media/gagandeep/2E92405C92402AA3/Work/Codes/PythonCodes/SparkLesson/DataFiles/Spark_for_Machine_Learning/Logistic_Regression/customer_churn.csv\"\n",
    "data = spark.read.csv(pathToFile, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- Names: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Total_Purchase: double (nullable = true)\n |-- Account_Manager: integer (nullable = true)\n |-- Years: double (nullable = true)\n |-- Num_Sites: double (nullable = true)\n |-- Onboard_date: timestamp (nullable = true)\n |-- Location: string (nullable = true)\n |-- Company: string (nullable = true)\n |-- Churn: integer (nullable = true)\n\n"
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                null|                null|0.16666666666666666|\n| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                null|                null| 0.3728852122772358|\n|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n\n"
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Names',\n 'Age',\n 'Total_Purchase',\n 'Account_Manager',\n 'Years',\n 'Num_Sites',\n 'Onboard_date',\n 'Location',\n 'Company',\n 'Churn']"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('features', 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lgr = LogisticRegression(labelCol='churn')\n",
    "fit_data = lgr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+-------------------+-------------------+\n|summary|              churn|         prediction|\n+-------+-------------------+-------------------+\n|  count|                648|                648|\n|   mean| 0.1743827160493827| 0.1419753086419753|\n| stddev|0.37973136418191983|0.34929443478456074|\n|    min|                0.0|                0.0|\n|    max|                1.0|                1.0|\n+-------+-------------------+-------------------+\n\n"
    }
   ],
   "source": [
    "fit_data.summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "pred_labels = fit_data.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------------+-----+--------------------+--------------------+----------+\n|            features|churn|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------------+--------------------+----------+\n|[28.0,8670.98,0.0...|    0|[8.37690824463786...|[0.99976993262070...|       0.0|\n|[28.0,9090.43,1.0...|    0|[1.36065164443400...|[0.79586558659857...|       0.0|\n|[29.0,5900.78,1.0...|    0|[4.09430656701825...|[0.98360594470461...|       0.0|\n|[29.0,9378.24,0.0...|    0|[5.06781644847795...|[0.99374324039972...|       0.0|\n|[29.0,9617.59,0.0...|    0|[4.63612658198162...|[0.99039791545319...|       0.0|\n|[30.0,6744.87,0.0...|    0|[3.71361521564169...|[0.97619147906467...|       0.0|\n|[30.0,7960.64,1.0...|    1|[3.56626558222971...|[0.97251554752342...|       0.0|\n|[30.0,10183.98,1....|    0|[2.90412169057924...|[0.94804981110381...|       0.0|\n|[30.0,10960.52,1....|    0|[2.25293399813136...|[0.90490331545542...|       0.0|\n|[31.0,8829.83,1.0...|    0|[4.62979550712163...|[0.99033752035268...|       0.0|\n|[31.0,11743.24,0....|    0|[7.08177763640432...|[0.99916042736351...|       0.0|\n|[31.0,12264.68,1....|    0|[3.45368132399573...|[0.96934073615194...|       0.0|\n|[32.0,6367.22,1.0...|    0|[3.43215503048348...|[0.96869448613334...|       0.0|\n|[32.0,10716.75,0....|    0|[4.69224101757446...|[0.99091713298191...|       0.0|\n|[32.0,11540.86,0....|    0|[7.50564403218121...|[0.99945033076554...|       0.0|\n|[32.0,12142.99,0....|    0|[5.96768741719300...|[0.99744638250782...|       0.0|\n|[32.0,12254.75,1....|    0|[2.56089908405265...|[0.92830232123108...|       0.0|\n|[32.0,12403.6,0.0...|    0|[5.97105751807056...|[0.99745495210551...|       0.0|\n|[33.0,4711.89,0.0...|    0|[6.27275870554705...|[0.99811653844601...|       0.0|\n|[33.0,14160.05,1....|    0|[5.46493080655286...|[0.99578520577352...|       0.0|\n+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "pred_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol = 'prediction', labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7358265241986173"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "auc = churn_eval.evaluate(pred_labels.predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lgr.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers = spark.read.csv(\"/media/gagandeep/2E92405C92402AA3/Work/Codes/PythonCodes/SparkLesson/DataFiles/Spark_for_Machine_Learning/Logistic_Regression/new_customers.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Names',\n 'Age',\n 'Total_Purchase',\n 'Account_Manager',\n 'Years',\n 'Num_Sites',\n 'Onboard_date',\n 'Location',\n 'Company']"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "new_customers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Names',\n 'Age',\n 'Total_Purchase',\n 'Account_Manager',\n 'Years',\n 'Num_Sites',\n 'Onboard_date',\n 'Location',\n 'Company',\n 'features']"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "test_new_customers = assembler.transform(new_customers)\n",
    "test_new_customers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = final_model.transform(test_new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- Names: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Total_Purchase: double (nullable = true)\n |-- Account_Manager: integer (nullable = true)\n |-- Years: double (nullable = true)\n |-- Num_Sites: double (nullable = true)\n |-- Onboard_date: timestamp (nullable = true)\n |-- Location: string (nullable = true)\n |-- Company: string (nullable = true)\n |-- features: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"
    }
   ],
   "source": [
    "new_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------------+----------+\n|         Company|prediction|\n+----------------+----------+\n|        King Ltd|       0.0|\n|   Cannon-Benson|       1.0|\n|Barron-Robertson|       1.0|\n|   Sexton-Golden|       1.0|\n|        Wood LLC|       0.0|\n|   Parks-Robbins|       1.0|\n+----------------+----------+\n\n"
    }
   ],
   "source": [
    "new_predictions.select('Company', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bite6955f2e74d64707b01ddae88c9da161",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}